{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python36\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from yolo import YOLO\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(yolo):\n",
    "    img = 'C:/Users/user/Desktop/3.png'\n",
    "    image = Image.open(img)\n",
    "    image_cv = cv2.cvtColor(np.asarray(image),cv2.COLOR_RGB2BGR)  \n",
    "      \n",
    "    image_class, image_box = yolo.detect_image(image)\n",
    "    person_box = []\n",
    "    hls_list=[]\n",
    "    for i, c in reversed(list(enumerate(image_class))):\n",
    "            predicted_class = yolo.class_names[c]\n",
    "            if predicted_class != 'person':\n",
    "               continue\n",
    "            person_box = image_box[i]\n",
    "            \n",
    "            top = max(0, np.floor(person_box[0] + 0.5).astype('int32'))            \n",
    "            left = max(0, np.floor(person_box[1] + 0.5).astype('int32'))\n",
    "            bottom = min(image.size[1], np.floor(person_box[2] + 0.5).astype('int32'))\n",
    "            right = min(image.size[0], np.floor(person_box[3] + 0.5).astype('int32'))            \n",
    "            x = int(left+(right-left)*1/6)\n",
    "            y = int(top+(bottom-top)*1/6)\n",
    "            w = int((right-left)*2/3)\n",
    "            h = int((bottom-top)*1/2)\n",
    "            crop_img= image_cv[y:y+h, x:x+w]\n",
    "            hls = cv2.cvtColor(crop_img,cv2.COLOR_BGR2HLS)\n",
    "            hls2 = hls[...,:,1]\n",
    "            hls3 = np.sum(hls2)\n",
    "            w,h=hls2.shape\n",
    "            hls_avg = int(hls3/(w*h))          \n",
    "            hls_list.append(hls_avg)\n",
    "            \n",
    "            print(hls_avg, hls_list)\n",
    "            cv2.imshow(str(hls_avg),crop_img)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_data/yolo.h5 model, anchors, and classes loaded.\n",
      "(416, 416, 3)\n",
      "47 [47]\n",
      "95 [47, 95]\n",
      "56 [47, 95, 56]\n",
      "122 [47, 95, 56, 122]\n",
      "176 [47, 95, 56, 122, 176]\n",
      "86 [47, 95, 56, 122, 176, 86]\n",
      "181 [47, 95, 56, 122, 176, 86, 181]\n",
      "119 [47, 95, 56, 122, 176, 86, 181, 119]\n",
      "137 [47, 95, 56, 122, 176, 86, 181, 119, 137]\n",
      "180 [47, 95, 56, 122, 176, 86, 181, 119, 137, 180]\n",
      "178 [47, 95, 56, 122, 176, 86, 181, 119, 137, 180, 178]\n",
      "98 [47, 95, 56, 122, 176, 86, 181, 119, 137, 180, 178, 98]\n",
      "76 [47, 95, 56, 122, 176, 86, 181, 119, 137, 180, 178, 98, 76]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(YOLO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
